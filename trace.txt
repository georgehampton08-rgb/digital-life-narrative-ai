============================= test session starts =============================
platform win32 -- Python 3.12.2, pytest-9.0.2, pluggy-1.6.0 -- C:\Users\georg\AppData\Local\Programs\Python\Python312\python.exe
cachedir: .pytest_cache
rootdir: C:\Users\georg\digital-life-narrative-ai
configfile: pyproject.toml
plugins: anyio-4.12.1
collecting ... collected 1 item

tests/test_ai_and_safety.py::TestLifeStoryAnalyzer::test_analyzer_handles_ai_error_gracefully FAILED [100%]

================================== FAILURES ===================================
_______ TestLifeStoryAnalyzer.test_analyzer_handles_ai_error_gracefully _______

self = <test_ai_and_safety.TestLifeStoryAnalyzer object at 0x0000024B84B75B50>
sample_memories = [Memory(id='82f219f4-507b-4cb2-abda-65aff21c1bda', content_hash=None, metadata_hash=None, source_platform=<SourcePlatf...ally_analyzed=False, location=None, people=[], tags=[], album_name=None, original_metadata={}, parse_warnings=[]), ...]

    def test_analyzer_handles_ai_error_gracefully(self, sample_memories: list[Memory]) -> None:
        """Analyzer handles partial AI failures gracefully."""
        mock_client = MagicMock()
        call_count = {"value": 0}
    
        def generate_side_effect(*args, **kwargs):
            call_count["value"] += 1
            if call_count["value"] <= 2:
                return AIResponse(text="Sample text", model="gemini-1.5-flash", total_tokens=100)
            raise AIClientError("Temporary failure")
    
        mock_client.generate.side_effect = generate_side_effect
        mock_client.generate_json.return_value = StructuredAIResponse(
            data={
                "chapters": [
                    {"title": "Test", "start_date": "2020-01-01", "end_date": "2020-12-31"}
                ]
            },
            raw_text="{}",
            model="gemini-1.5-flash",
            tokens_used=100,
            parse_success=True,
        )
        mock_client.generate_structured.return_value = StructuredAIResponse(
            data={
                "chapters": [
                    {"title": "Test", "start_date": "2020-01-01", "end_date": "2020-12-31"}
                ]
            },
            raw_text="{}",
            model="gemini-1.5-flash",
            tokens_used=100,
            parse_success=True,
        )
        mock_client.is_available.return_value = True
    
        analyzer = LifeStoryAnalyzer(client=mock_client)
        # Should not crash, may return partial result
        try:
>           result = analyzer.analyze(sample_memories)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\test_ai_and_safety.py:303: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.ai.analyzer.LifeStoryAnalyzer object at 0x0000024B84BA6300>
memories = [Memory(id='82f219f4-507b-4cb2-abda-65aff21c1bda', content_hash=None, metadata_hash=None, source_platform=<SourcePlatf...ally_analyzed=False, location=None, people=[], tags=[], album_name=None, original_metadata={}, parse_warnings=[]), ...]
config = None, progress_callback = None

    def analyze(
        self,
        memories: list[Memory],
        config: AnalysisConfig | None = None,
        progress_callback: Callable[[AnalysisProgress], None] | None = None,
    ) -> LifeStoryReport:
        """Analyze memories and generate a life story report.
    
        Args:
            memories: List of Memory objects to analyze.
            config: Analysis configuration (depth, models, etc.)
            progress_callback: Optional callback for progress updates.
    
        Returns:
            Complete LifeStoryReport with chapters and narratives.
        """
        analysis_config = config or AnalysisConfig()
        start_time = datetime.now()
        self.debug_analysis_log = [] # Reset for new run
    
        def report_progress(stage: str, percent: float, message: str = "", chapter: str | None = None) -> None:
            if progress_callback:
                progress = AnalysisProgress(
                    stage=stage,
                    percent=percent,
                    message=message,
                    chapter=chapter,
                    elapsed_seconds=(datetime.now() - start_time).total_seconds()
                )
                progress_callback(progress)
    
        report_progress("Initializing", 0.0)
    
        # Validate input
        if len(memories) < self.MIN_MEMORIES_FOR_ANALYSIS:
            raise InsufficientDataError(
                f"Need at least {self.MIN_MEMORIES_FOR_ANALYSIS} memories, got {len(memories)}"
            )
    
        # Check against warning threshold
        ai_cfg = self.client._config.ai
>       if len(memories) > ai_cfg.warn_on_large_dataset_threshold:
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: '>' not supported between instances of 'int' and 'MagicMock'

src\ai\analyzer.py:205: TypeError
=========================== short test summary info ===========================
FAILED tests/test_ai_and_safety.py::TestLifeStoryAnalyzer::test_analyzer_handles_ai_error_gracefully - TypeError: '>' not supported between instances of 'int' and 'MagicMock'
============================== 1 failed in 0.15s ==============================
