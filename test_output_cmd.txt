============================= test session starts =============================
platform win32 -- Python 3.12.2, pytest-9.0.2, pluggy-1.6.0 -- C:\Users\georg\AppData\Local\Programs\Python\Python312\python.exe
cachedir: .pytest_cache
rootdir: C:\Users\georg\digital-life-narrative-ai
configfile: pyproject.toml
plugins: anyio-4.12.1
collecting ... collected 1 item

tests/test_ai_and_safety.py::TestLifeStoryAnalyzer::test_analyzer_progress_callback FAILED [100%]

================================== FAILURES ===================================
____________ TestLifeStoryAnalyzer.test_analyzer_progress_callback ____________

self = <test_ai_and_safety.TestLifeStoryAnalyzer object at 0x0000024F1FF68F80>
sample_memories = [Memory(id='0f15faaf-4068-4b10-87a1-7fefef42b619', content_hash=None, metadata_hash=None, source_platform=<SourcePlatf...ally_analyzed=False, location=None, people=[], tags=[], album_name=None, original_metadata={}, parse_warnings=[]), ...]
mock_ai_client = <MagicMock id='2538861927392'>

    def test_analyzer_progress_callback(
        self, sample_memories: list[Memory], mock_ai_client: MagicMock
    ) -> None:
        """Progress callback is called during analysis."""
        progress_calls = []
    
        def callback(progress):
            progress_calls.append(progress)
    
        analyzer = LifeStoryAnalyzer(client=mock_ai_client)
>       analyzer.analyze(sample_memories, progress_callback=callback)

tests\test_ai_and_safety.py:257: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.ai.analyzer.LifeStoryAnalyzer object at 0x0000024F1FF68B60>
memories = [Memory(id='0f15faaf-4068-4b10-87a1-7fefef42b619', content_hash=None, metadata_hash=None, source_platform=<SourcePlatf...ally_analyzed=False, location=None, people=[], tags=[], album_name=None, original_metadata={}, parse_warnings=[]), ...]
config = None
progress_callback = <function TestLifeStoryAnalyzer.test_analyzer_progress_callback.<locals>.callback at 0x0000024F1FF70D60>

    def analyze(
        self,
        memories: list[Memory],
        config: AnalysisConfig | None = None,
        progress_callback: Callable[[AnalysisProgress], None] | None = None,
    ) -> LifeStoryReport:
        """Analyze memories and generate a life story report.
    
        Args:
            memories: List of Memory objects to analyze.
            config: Analysis configuration (depth, models, etc.)
            progress_callback: Optional callback for progress updates.
    
        Returns:
            Complete LifeStoryReport with chapters and narratives.
        """
        analysis_config = config or AnalysisConfig()
        start_time = datetime.now()
        self.debug_analysis_log = [] # Reset for new run
    
        def report_progress(stage: str, percent: float, message: str = "", chapter: str | None = None) -> None:
            if progress_callback:
                progress = AnalysisProgress(
                    stage=stage,
                    percent=percent,
                    message=message,
                    chapter=chapter,
                    elapsed_seconds=(datetime.now() - start_time).total_seconds()
                )
                progress_callback(progress)
    
        report_progress("Initializing", 0.0)
    
        # Validate input
        if len(memories) < self.MIN_MEMORIES_FOR_ANALYSIS:
            raise InsufficientDataError(
                f"Need at least {self.MIN_MEMORIES_FOR_ANALYSIS} memories, got {len(memories)}"
            )
    
        # Check against warning threshold
        ai_cfg = self.client._config.ai
        if len(memories) > ai_cfg.warn_on_large_dataset_threshold:
            self._logger.warning(
                f"Large dataset detected ({len(memories)} memories). "
                f"Analysis may take longer and sampling will be more aggressive."
            )
    
        # Build timeline
        report_progress("Building Timeline", 5.0)
        timeline = Timeline(memories)
        stats = timeline.compute_statistics()
    
        # Phase 1: Granular Analytics (Platform/Gaps/Patterns)
        report_progress("Deep Analytical Pass", 15.0)
>       platform_insights = self._analyze_platforms(timeline, analysis_config)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

src\ai\analyzer.py:218: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.ai.analyzer.LifeStoryAnalyzer object at 0x0000024F1FF68B60>
timeline = <src.core.timeline.Timeline object at 0x0000024F1FFA2E10>
config = AnalysisConfig(depth=<DepthMode.DEEP: 'deep'>, vision_model='gemini-2.0-flash-exp', narrative_model='gemini-1.5-pro', ...clude_gap_analysis=True, detect_patterns=True, narrative_style='warm', privacy_level='standard', fail_on_partial=False)

    def _analyze_platforms(
        self,
        timeline: Timeline,
        config: AnalysisConfig
    ) -> list[PlatformBehaviorInsight]:
        """Analyze how different platforms are used for documentation."""
        if not config.include_platform_analysis:
            return []
    
        stats = timeline.compute_statistics()
>       if not stats.platform_counts:
               ^^^^^^^^^^^^^^^^^^^^^

src\ai\analyzer.py:684: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = TimelineStatistics(total_memories=31, dated_memories=29, undated_memories=2, date_range=DateRange(start=datetime.date(...ne, following_period=None, severity='critical', possible_causes=['Year-long gap - possible life event or lost data'])])
item = 'platform_counts'

    def __getattr__(self, item: str) -> Any:
        private_attributes = object.__getattribute__(self, '__private_attributes__')
        if item in private_attributes:
            attribute = private_attributes[item]
            if hasattr(attribute, '__get__'):
                return attribute.__get__(self, type(self))  # type: ignore
    
            try:
                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items
                return self.__pydantic_private__[item]  # type: ignore
            except KeyError as exc:
                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
        else:
            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.
            # See `BaseModel.__repr_args__` for more details
            try:
                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')
            except AttributeError:
                pydantic_extra = None
    
            if pydantic_extra and item in pydantic_extra:
                return pydantic_extra[item]
            else:
                if hasattr(self.__class__, item):
                    return super().__getattribute__(item)  # Raises AttributeError if appropriate
                else:
                    # this is the current error
>                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E                   AttributeError: 'TimelineStatistics' object has no attribute 'platform_counts'

..\AppData\Local\Programs\Python\Python312\Lib\site-packages\pydantic\main.py:1026: AttributeError
=========================== short test summary info ===========================
FAILED tests/test_ai_and_safety.py::TestLifeStoryAnalyzer::test_analyzer_progress_callback - AttributeError: 'TimelineStatistics' object has no attribute 'platform_counts'
============================== 1 failed in 0.17s ==============================
