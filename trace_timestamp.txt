============================= test session starts =============================
platform win32 -- Python 3.12.2, pytest-9.0.2, pluggy-1.6.0 -- C:\Users\georg\AppData\Local\Programs\Python\Python312\python.exe
cachedir: .pytest_cache
rootdir: C:\Users\georg\digital-life-narrative-ai
configfile: pyproject.toml
plugins: anyio-4.12.1
collecting ... collected 1 item

tests/test_ai_and_safety.py::TestLifeStoryAnalyzer::test_analyzer_progress_callback FAILED [100%]

================================== FAILURES ===================================
____________ TestLifeStoryAnalyzer.test_analyzer_progress_callback ____________

self = <test_ai_and_safety.TestLifeStoryAnalyzer object at 0x000002E9F8A818E0>
sample_memories = [Memory(id='c4c6310e-aff4-4ec7-ab25-62a64198193c', content_hash=None, metadata_hash=None, source_platform=<SourcePlatf...ally_analyzed=False, location=None, people=[], tags=[], album_name=None, original_metadata={}, parse_warnings=[]), ...]
mock_ai_client = <MagicMock id='3203922402944'>

    def test_analyzer_progress_callback(
        self, sample_memories: list[Memory], mock_ai_client: MagicMock
    ) -> None:
        """Progress callback is called during analysis."""
        progress_calls = []
    
        def callback(progress):
            progress_calls.append(progress)
    
        analyzer = LifeStoryAnalyzer(client=mock_ai_client)
>       analyzer.analyze(sample_memories, progress_callback=callback)

tests\test_ai_and_safety.py:257: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.ai.analyzer.LifeStoryAnalyzer object at 0x000002E9F8A80EC0>
memories = [Memory(id='c4c6310e-aff4-4ec7-ab25-62a64198193c', content_hash=None, metadata_hash=None, source_platform=<SourcePlatf...ally_analyzed=False, location=None, people=[], tags=[], album_name=None, original_metadata={}, parse_warnings=[]), ...]
config = None
progress_callback = <function TestLifeStoryAnalyzer.test_analyzer_progress_callback.<locals>.callback at 0x000002E9F8A85260>

    def analyze(
        self,
        memories: list[Memory],
        config: AnalysisConfig | None = None,
        progress_callback: Callable[[AnalysisProgress], None] | None = None,
    ) -> LifeStoryReport:
        """Analyze memories and generate a life story report.
    
        Args:
            memories: List of Memory objects to analyze.
            config: Analysis configuration (depth, models, etc.)
            progress_callback: Optional callback for progress updates.
    
        Returns:
            Complete LifeStoryReport with chapters and narratives.
        """
        analysis_config = config or AnalysisConfig()
        start_time = datetime.now()
        self.debug_analysis_log = [] # Reset for new run
    
        def report_progress(stage: str, percent: float, message: str = "", chapter: str | None = None) -> None:
            if progress_callback:
                progress = AnalysisProgress(
                    stage=stage,
                    percent=percent,
                    message=message,
                    chapter=chapter,
                    elapsed_seconds=(datetime.now() - start_time).total_seconds()
                )
                progress_callback(progress)
    
        report_progress("Initializing", 0.0)
    
        # Validate input
        if len(memories) < self.MIN_MEMORIES_FOR_ANALYSIS:
            raise InsufficientDataError(
                f"Need at least {self.MIN_MEMORIES_FOR_ANALYSIS} memories, got {len(memories)}"
            )
    
        # Check against warning threshold
        ai_cfg = self._ai_config
        threshold = getattr(ai_cfg, "warn_on_large_dataset_threshold", 500)
        if not isinstance(threshold, (int, float)):
            threshold = 500
    
        if len(memories) > threshold:
            self._logger.warning(
                f"Large dataset detected ({len(memories)} memories). "
                f"Analysis may take longer and sampling will be more aggressive."
            )
    
        # Build timeline
        report_progress("Building Timeline", 5.0)
        timeline = Timeline(memories)
        stats = timeline.compute_statistics()
    
        # Phase 1: Granular Analytics (Platform/Gaps/Patterns)
        report_progress("Deep Analytical Pass", 15.0)
        platform_insights = self._analyze_platforms(timeline, analysis_config)
>       data_gaps = self._analyze_gaps(timeline, analysis_config)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

src\ai\analyzer.py:242: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.ai.analyzer.LifeStoryAnalyzer object at 0x000002E9F8A80EC0>
timeline = <src.core.timeline.Timeline object at 0x000002E9F8AB2F00>
config = AnalysisConfig(depth=<DepthMode.DEEP: 'deep'>, vision_model='gemini-2.0-flash-exp', narrative_model='gemini-1.5-pro', ...clude_gap_analysis=True, detect_patterns=True, narrative_style='warm', privacy_level='standard', fail_on_partial=False)

    def _analyze_gaps(
        self,
        timeline: Timeline,
        config: AnalysisConfig
    ) -> list[DataGap]:
        """Analyze and explain timeline silences."""
        if not config.include_gap_analysis:
            return []
    
        raw_gaps = timeline.detect_gaps(min_gap_days=30)
        if not raw_gaps:
            return []
    
        significant_gaps = sorted(raw_gaps, key=lambda g: g.duration_days, reverse=True)[:8]
    
        gaps_data_lines = []
        gap_context_parts = []
    
        for gap in significant_gaps:
            gaps_data_lines.append(f"Gap: {gap.start_date} to {gap.end_date} ({gap.duration_days} days)")
    
            before = timeline.get_memories_near(gap.start_date, limit=3)
            after = timeline.get_memories_near(gap.end_date, limit=3)
    
            ctx = {
                "gap": f"{gap.start_date} to {gap.end_date}",
                "context_before": [m.to_ai_payload(config.privacy_level) for m in before],
                "context_after": [m.to_ai_payload(config.privacy_level) for m in after]
            }
            gap_context_parts.append(json.dumps(ctx))
    
        system, user = GAP_ANALYSIS_PROMPT.render(
            gaps_data="\n".join(gaps_data_lines),
            gap_context="\n".join(gap_context_parts),
>           timeline_summary=prepare_timeline_summary(timeline.memories)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        )

src\ai\analyzer.py:794: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

memories = [Memory(id='c4c6310e-aff4-4ec7-ab25-62a64198193c', content_hash=None, metadata_hash=None, source_platform=<SourcePlatf...ally_analyzed=False, location=None, people=[], tags=[], album_name=None, original_metadata={}, parse_warnings=[]), ...]

    def prepare_timeline_summary(memories: list["Memory"]) -> str:
        """Generate statistical summary for prompt context.
    
        Creates a text summary of the timeline including yearly distribution,
        platform breakdown, location patterns, and media types.
    
        Args:
            memories: List of Memory objects to summarize.
    
        Returns:
            Multi-line string summary suitable for prompt insertion.
    
        Example:
            >>> summary = prepare_timeline_summary(memories)
            >>> print(summary)
            ### Yearly Distribution
            2020: 500 memories
            2021: 750 memories
            ...
        """
        if not memories:
            return "No memories in timeline."
    
        lines = []
    
        # Yearly distribution
        years: Counter[int] = Counter()
        for m in memories:
>           if m.timestamp:
               ^^^^^^^^^^^

src\ai\prompts.py:980: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = Memory(id='c4c6310e-aff4-4ec7-ab25-62a64198193c', content_hash=None, metadata_hash=None, source_platform=<SourcePlatfo..., visually_analyzed=False, location=None, people=[], tags=[], album_name=None, original_metadata={}, parse_warnings=[])
item = 'timestamp'

    def __getattr__(self, item: str) -> Any:
        private_attributes = object.__getattribute__(self, '__private_attributes__')
        if item in private_attributes:
            attribute = private_attributes[item]
            if hasattr(attribute, '__get__'):
                return attribute.__get__(self, type(self))  # type: ignore
    
            try:
                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items
                return self.__pydantic_private__[item]  # type: ignore
            except KeyError as exc:
                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
        else:
            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.
            # See `BaseModel.__repr_args__` for more details
            try:
                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')
            except AttributeError:
                pydantic_extra = None
    
            if pydantic_extra and item in pydantic_extra:
                return pydantic_extra[item]
            else:
                if hasattr(self.__class__, item):
                    return super().__getattribute__(item)  # Raises AttributeError if appropriate
                else:
                    # this is the current error
>                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E                   AttributeError: 'Memory' object has no attribute 'timestamp'

..\AppData\Local\Programs\Python\Python312\Lib\site-packages\pydantic\main.py:1026: AttributeError
=========================== short test summary info ===========================
FAILED tests/test_ai_and_safety.py::TestLifeStoryAnalyzer::test_analyzer_progress_callback - AttributeError: 'Memory' object has no attribute 'timestamp'
============================== 1 failed in 0.18s ==============================
